{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "In this tutorial, we are going to look at how AquilaDB vector database can help efficient Semantic Retrieval with Google's Universal Sentence Encoder (USE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is following the same idea as described [in latest post on Universal Sentence Encoder at Google AI blog](https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html). One difference is that, we are using a model which is different from the one mentioned in the blog post. We use `universal-sentence-encoder-large` which belongs the same `USE` family. We encourage you to read that blog before proceeding. Because it is very useful to get a context on what we are going to do below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an image taken from that blog post. A recommended pipeline for textual similarity. `AquilaDB` will cover `pre-encoded Candidates` data store and `ANN search` modules in this pipeline. Cool.. Right?\n",
    "\n",
    "![A prototypical semantic retrieval pipeline, used for textual similarity](https://1.bp.blogspot.com/-q1g13xLR-9E/XSi8ZewIXzI/AAAAAAAAETQ/Oek9K51ZrAQvbZL3t3rme5HcegzCNm98QCEwYBhgL/s640/image1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import required modules\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained encoder\n",
    "We need to load pretrained USE model from Tensorflow Hub. We use this model to encode our sentances before sending it to AquilaDB for indexing and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "\n",
    "# load Universal Sentence Encoder module from tensor hub\n",
    "embed_module = hub.Module(use_module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our loaded model with some random texts before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some test sentanaces\n",
    "test_messages = [\"AquilaDB is a Resillient, Replicated, Decentralized, Host neutral storage for Feature Vectors along with Document Metadata.\", \n",
    "            \"Do k-NN retrieval from anywhere, even from the darkest rifts of Aquila (in progress). It is easy to setup and scales as the universe expands.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed our text array to model for embedding. Don't forget to wrap the embedding logic into a method to reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to generate embedding for input array of sentances\n",
    "def generate_embeddings (messages_in):\n",
    "    # generate embeddings\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(embed_module(messages_in))\n",
    "        \n",
    "    return message_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00570544  0.01024008  0.04416275 ...  0.03282805 -0.01723128\n",
      "   0.00956334]\n",
      " [ 0.0124177   0.09862255  0.06958324 ... -0.00700251  0.02332876\n",
      "  -0.09377097]]\n"
     ]
    }
   ],
   "source": [
    "# print generated embeddings\n",
    "print(generate_embeddings(test_messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we were able to encode out random texts into corresponding sentance embedding with `USE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load actual data\n",
    "We will be loading some text from a text file. This is a small wiki article set in plain text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('article_set.txt', 'r') as file_in:\n",
    "    lines = file_in.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write some helper functions. These functions will help us communicate with AquilaDB. You don't have to worry about this part now. Just keep it as is except for the IP address `192.168.1.100`. Replace that with the IP address where your AquilaDB installation is. Most probably, it is the same machine you are using now - then give `localhost` as address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to generate documents\n",
    "\n",
    "import grpc\n",
    "\n",
    "import vecdb_pb2\n",
    "import vecdb_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('192.168.1.100:50051')\n",
    "stub = vecdb_pb2_grpc.VecdbServiceStub(channel)\n",
    "\n",
    "# API interface to add documents to AquilaDB\n",
    "def addDocuments (documents_in):\n",
    "    response = stub.addDocuments(vecdb_pb2.addDocRequest(documents=documents_in))\n",
    "    return response\n",
    "\n",
    "\n",
    "import base64\n",
    "import json\n",
    "\n",
    "# helper function to convert native data to API friendly data\n",
    "def convertDocuments(vector, document):\n",
    "    return {\n",
    "            \"vector\": {\n",
    "                \"e\": vector\n",
    "            },\n",
    "            \"b64data\": json.dumps(document, separators=(',', ':')).encode('utf-8')\n",
    "        }\n",
    "\n",
    "\n",
    "# API interface to get nearest documents from AquilaDB\n",
    "def getNearest (matrix_in, k_in):\n",
    "    response = stub.getNearest(vecdb_pb2.getNearestRequest(matrix=matrix_in, k=k_in))\n",
    "    return response\n",
    "\n",
    "\n",
    "# helper function to convert native data to API friendly data\n",
    "def convertMatrix(vector):\n",
    "    return [{\n",
    "            \"e\": vector\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send documents to AquilaDB for indexing\n",
    "As mentioned previously, we need to store pre encoded candidates in a vector database to perform semantic similarity retrieval later. So, what we are going to do here is to take each line from wiki articles, encode them with `USE` model, attach the original wiki text with the resulting vector as metadata and send them to AquilaDB for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 1 inserted: 199\n",
      "index: 2 inserted: 178\n",
      "index: 3 inserted: 144\n",
      "index: 4 inserted: 200\n",
      "index: 5 inserted: 198\n",
      "index: 6 inserted: 199\n",
      "index: 7 inserted: 197\n",
      "index: 8 inserted: 185\n",
      "index: 9 inserted: 196\n",
      "index: 10 inserted: 170\n",
      "index: 11 inserted: 200\n",
      "index: 12 inserted: 193\n",
      "index: 13 inserted: 197\n",
      "index: 14 inserted: 194\n",
      "index: 15 inserted: 190\n",
      "index: 16 inserted: 200\n",
      "index: 17 inserted: 151\n",
      "index: 18 inserted: 200\n",
      "index: 19 inserted: 195\n",
      "index: 20 inserted: 200\n",
      "index: 21 inserted: 183\n",
      "index: 22 inserted: 200\n",
      "index: 23 inserted: 186\n",
      "index: 24 inserted: 198\n",
      "index: 25 inserted: 200\n",
      "index: 26 inserted: 197\n",
      "index: 27 inserted: 200\n",
      "index: 28 inserted: 163\n",
      "index: 29 inserted: 198\n",
      "index: 30 inserted: 179\n",
      "index: 31 inserted: 200\n",
      "index: 32 inserted: 190\n",
      "index: 33 inserted: 188\n",
      "index: 34 inserted: 195\n",
      "index: 35 inserted: 198\n",
      "index: 36 inserted: 200\n",
      "index: 37 inserted: 194\n",
      "index: 38 inserted: 184\n",
      "index: 39 inserted: 200\n",
      "index: 40 inserted: 192\n",
      "index: 41 inserted: 197\n",
      "index: 42 inserted: 199\n",
      "index: 43 inserted: 198\n",
      "index: 44 inserted: 198\n",
      "index: 45 inserted: 192\n",
      "index: 46 inserted: 195\n",
      "index: 47 inserted: 198\n",
      "index: 48 inserted: 200\n",
      "index: 49 inserted: 199\n",
      "index: 50 inserted: 200\n",
      "index: 51 inserted: 196\n",
      "index: 52 inserted: 200\n",
      "index: 53 inserted: 198\n",
      "index: 54 inserted: 200\n",
      "index: 55 inserted: 200\n",
      "index: 56 inserted: 186\n",
      "index: 57 inserted: 196\n",
      "index: 58 inserted: 180\n",
      "index: 59 inserted: 198\n",
      "index: 60 inserted: 198\n",
      "index: 61 inserted: 199\n",
      "index: 62 inserted: 196\n",
      "index: 63 inserted: 196\n",
      "index: 64 inserted: 199\n",
      "index: 65 inserted: 196\n",
      "index: 66 inserted: 197\n",
      "index: 67 inserted: 188\n",
      "index: 68 inserted: 171\n",
      "index: 69 inserted: 200\n",
      "index: 70 inserted: 154\n",
      "index: 71 inserted: 196\n",
      "index: 72 inserted: 196\n",
      "index: 73 inserted: 193\n",
      "index: 74 inserted: 181\n",
      "index: 75 inserted: 199\n",
      "index: 76 inserted: 196\n",
      "index: 77 inserted: 186\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# set a batch length\n",
    "batch_len = 200\n",
    "# counter to init batch sending of documents\n",
    "counter = 0\n",
    "# to keep generated documents\n",
    "docs_gen = []\n",
    "# to keep lines batch\n",
    "lbatch = []\n",
    "\n",
    "for line in lines:\n",
    "    lbatch.append(line)\n",
    "    if len(lbatch) == batch_len:\n",
    "        counter = counter + 1\n",
    "        # generate embeddings\n",
    "        vectors = generate_embeddings(lbatch)\n",
    "        for i in range(len(vectors)):\n",
    "            docs_gen.append(convertDocuments(vectors[i], {\"text\": lbatch[i]}))\n",
    "        # add documents to AquilaDB\n",
    "        response = addDocuments(docs_gen)\n",
    "        print(\"index: \"+str(counter), \"inserted: \"+str(len(response._id)))\n",
    "        docs_gen = []\n",
    "        lbatch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the database\n",
    "Now, we need to retrieve semantically similar sentance to our input query from the database. It is straight forward. Just encode the query text with the same `USE` model and then perform k-NN query on the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to query for nearest neighbours\n",
    "def query_nn (query):\n",
    "    query = [query]\n",
    "    vector = generate_embeddings(query)[0]\n",
    "    \n",
    "    converted_vector = convertMatrix(vector)\n",
    "    nearest_docs_result = getNearest(converted_vector, 1)\n",
    "    nearest_docs_result = json.loads(nearest_docs_result.documents)\n",
    "    \n",
    "    return nearest_docs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swans are birds of the family Anatidae, which also includes geese and ducks. Swans are grouped with the closely related geese in the subfamily Anserinae where they form the tribe Cygnini. Sometimes, they are considered a distinct subfamily, Cygninae. Swans usually mate for life, though 'divorce' does sometimes occur, particularly following nesting failure. The number of eggs in each clutch ranges from three to eight.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try an example query.\n",
    "\n",
    "print(query_nn('what are the subfamilies of duck')[0]['doc']['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for this tutorial. Thanks, happy hacking..!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "created with ❤️ a-mma.indic (a_മ്മ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
